{"cells":[{"cell_type":"markdown","metadata":{"id":"E6oq8m1yjrBr"},"source":["# Training an Autoencoder for Image Segmentation"]},{"cell_type":"markdown","metadata":{"id":"SVtdO3qtHKj8"},"source":["Organise data directories containing training data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0eE2aYn4HDwa"},"outputs":[],"source":["import os\n","import sys\n","import h5py  # !pip install pyyaml h5py\n","import numpy as np\n","from tqdm.notebook import tqdm\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","# Automatically reload imported programmes\n","%load_ext autoreload\n","%autoreload 2\n","\n","# Locate data\n","data_file = 'tomograms2D/all'  # No leading/trailing `/`\n","exp_name = 'all-2D-whole-ae'\n","\n","# Directories (ammend as necessary)\n","root_dir = '/content/gdrive/MyDrive/IDSAI/PROOF/filament-segmentation'\n","os.chdir(root_dir)  # Move to root_dir\n","sys.path.insert(0, root_dir)\n","\n","# Add data to root directory and locate JSON file\n","data_dir = os.path.join(root_dir, 'data/' + data_file)\n","image_path = os.path.join(data_dir, 'png-original')\n","masks_path = os.path.join(data_dir, 'png-masks/semantic/*.png')\n","\n","# New training and validation files\n","train_dir = os.path.join(root_dir, 'data/databases/' + exp_name + '/train')\n","valid_dir = os.path.join(root_dir, 'data/databases/' + exp_name + '/valid')\n","\n","# Checkpoints\n","checkpoint_dir = os.path.join(root_dir, 'checkpoints/' + exp_name)\n","if not os.path.exists(checkpoint_dir):\n","    os.makedirs(checkpoint_dir)\n","checkpoint_path = os.path.join(checkpoint_dir, 'cp-{epoch:04d}.h5')\n","best_weights_path = os.path.join(checkpoint_dir, 'ae-best-weights.h5')\n","\n","# Figure Outputs\n","fig_dir = os.path.join(root_dir, 'outputs/ae-train-' + exp_name)\n","os.makedirs(fig_dir, exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"KfF-BJQpJ2XU"},"source":["Assert GPU/TPU and RAM capability."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kqp1c0wdt2rF"},"outputs":[],"source":["%%script false\n","# GPU info\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","   print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Btu9gegjuAPw"},"outputs":[],"source":["%%script false\n","# TPU initialisation for tensorflow 2.X\n","resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n","tf.config.experimental_connect_to_cluster(resolver)\n","tf.tpu.experimental.initialize_tpu_system(resolver)\n","print(\"All devices: \", tf.config.list_logical_devices('TPU'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RYbH70JJtJiE"},"outputs":[],"source":["%%script false\n","## RAM availability\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"markdown","metadata":{"id":"-EMTSZfGGvMk"},"source":["## Load data and model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MV7YwZNnD4HR"},"outputs":[],"source":["print('\\nLoading data...')\n","if not os.path.exists(train_dir) and not os.path.exists(valid_dir):\n","\n","    from loader import augment_data, get_data\n","    train_imgs, train_msks, valid_imgs, valid_msks, _, _ = \\\n","        get_data(path_train_imgs=image_path,\n","                 path_train_msks=masks_path,\n","                 path_valid_imgs='',\n","                 path_valid_msks='',\n","                 train_frac=0.8,\n","                 valid_frac=0.1,\n","                 image_size=[256, 256],\n","                 num_images_per_original=1,\n","                 num_duplicates_before_augmenting=30,\n","                 )\n","    train_set, valid_set = augment_data(\n","        train_imgs, train_msks, valid_imgs, valid_msks, batch_size,\n","    )\n","\n","    \n","\n","    tf.data.experimental.save(train_set, train_dir)\n","    tf.data.experimental.save(valid_set, valid_dir)\n","    print('Data processed, loaded and saved.')\n","else:\n","    train_set = tf.data.experimental.load(train_dir)\n","    valid_set = tf.data.experimental.load(valid_dir)\n","    print('Data loaded from file.')\n","print('Training set length: ', len(train_set))\n","print('Validation set length: ', len(valid_set))"]},{"cell_type":"markdown","metadata":{"id":"tbrgiKwwIQKT"},"source":["Instantiate model and load the `best-weights.h5` from previous training sessions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mu5o9IU7IHCs"},"outputs":[],"source":["from models import Autoencoder, ae_train_step, ae_test_step\n","\n","# Instantiate model\n","model = Autoencoder(autoencoder_lr=lr)\n","\n","# Call model to build and initialise weights\n","data_shape = list(train_set)[0][0].shape\n","model(np.zeros(data_shape))\n","\n","if os.path.exists(best_weights_path):\n","    model.load_weights(best_weights_path)\n","    print('Model loaded with previous `best weights`.')\n","\n","else:\n","    print('Initialising model randomly.')"]},{"cell_type":"markdown","metadata":{"id":"GGhe1-BUITMu"},"source":["## Training Routine"]},{"cell_type":"markdown","metadata":{"id":"v8ErMPtHKAPh"},"source":["Choose network parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G6jeXNsuDUrq"},"outputs":[],"source":["lr = 0.0001\n","epoch_freq = 10\n","num_epochs = 250\n","batch_size = 10"]},{"cell_type":"markdown","source":["Iterate training."],"metadata":{"id":"VWljORP_4C_s"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXGQ4kbwIVw1"},"outputs":[],"source":["# Iterate training session\n","tlosses = []    \n","vlosses = []\n","elosses = []\n","best_epoch = 0\n","save_weight_improvements = False\n","for epoch in tqdm(range(num_epochs)):\n","    print('Epoch: ', epoch)\n","    \n","    if epoch == 0:\n","        print('\\nSaving model weights at epoch {:d}.'.format(epoch))\n","        if save_weight_improvements:\n","            model.save_weights(checkpoint_path.format(epoch=epoch),\n","                            save_format='h5',\n","                            )\n","        model.save_weights(best_weights_path, save_format='h5')\n","\n","    # Take a training step\n","    for data in train_set:\n","        tlosses.extend(np.array(ae_train_step(data, model)).flatten())\n","        end_data = data\n","\n","    # Compute validation loss and checkpoint every `epoch_freq` epochs\n","    if epoch % epoch_freq == 0:\n","        evls_train, _, imgs_train, msks_train = ae_test_step(end_data, model)\n","        accum_valid_loss = []\n","        for x_valid in valid_set:\n","            evls_valid, valid_loss, imgs_valid, msks_valid = \\\n","                                                ae_test_step(x_valid, model)\n","            valid_loss = np.array(valid_loss).flatten()\n","            accum_valid_loss.extend(valid_loss)\n","        valid_loss = np.mean(accum_valid_loss)\n","        if epoch > 0 and all(valid_loss < past_loss for past_loss in vlosses):\n","            best_epoch = epoch\n","            print('\\nValidation loss decrease from {:.3f} to {:.3f} over '\n","                  'epochs {:d} to {:d}.'\n","                  .format(vlosses[-1], valid_loss, epoch - epoch_freq, epoch)\n","            )\n","            print('Saving initial model weights at epoch {:d}.'.format(epoch))\n","            if save_weight_improvements:\n","                model.save_weights(checkpoint_path.format(epoch=epoch),\n","                                save_format='h5',\n","                                )\n","            model.save_weights(best_weights_path, save_format='h5')\n","        else:\n","            print('\\nNo validation loss improvement at epoch {:d}.'\n","                  .format(epoch))\n","            \n","        vlosses.append(valid_loss)\n","        print('Training loss: ', tlosses[-1])\n","\n","        # Graphical Output\n","        for i in range(min(batch_size, 1)):\n","\n","            print('\\n')\n","            print('Training Results.')\n","            print('Range of mask energies: {min:.3f} to {max:.3f}'\n","                  .format(min=np.min(evls_train[i, :, :, 0]),\n","                          max=np.max(evls_train[i, :, :, 0]),\n","                          )\n","                  )\n","            fig, ax = plt.subplots(1, 3, constrained_layout=True)\n","            ax[0].imshow(imgs_train[i, :, :, 0])\n","            ax[0].set_title('Input')\n","            ax[1].imshow(msks_train[i, :, :, 0])\n","            ax[1].set_title('Mask')\n","            ax[2].imshow(evls_train[i, :, :, 0])\n","            ax[2].set_title('Prediction')\n","            plt.show()\n","            fig.savefig(\n","                os.path.join(\n","                    fig_dir, 'train_bnum{:02d}_epoch{:04d}'.format(i, epoch),\n","                    )\n","                )\n","\n","            print('\\n')\n","            print('Validation Results.')\n","            print('Range of mask energies: {min:.3f} to {max:.3f}'\n","                  .format(min=np.min(evls_valid[i, :, :, 0]),\n","                          max=np.max(evls_valid[i, :, :, 0]),\n","                          )\n","                  )\n","            fig, ax = plt.subplots(1, 3, constrained_layout=True)\n","            ax[0].imshow(imgs_valid[i, :, :, 0])\n","            ax[0].set_title('Input')\n","            ax[1].imshow(msks_valid[i, :, :, 0])\n","            ax[1].set_title('Mask')\n","            ax[2].imshow(evls_valid[i, :, :, 0])\n","            ax[2].set_title('Prediction')\n","            plt.show()\n","            fig.savefig(\n","                os.path.join(fig_dir,\n","                             'valid_bnum{:02d}_epoch{:04d}'.format(i, epoch),\n","                             )\n","                        )\n","            print('\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQsGTswbDV5a"},"outputs":[],"source":["# Generate loss plot\n","fig, ax = plt.subplots(1, 1)\n","\n","xt = np.linspace(\n","    0, num_epochs - (1/len(train_set)), num_epochs * len(train_set)\n","    )\n","\n","xv = np.linspace(\n","    0, num_epochs - epoch_freq, int(num_epochs / epoch_freq)\n","    )\n","\n","ax.plot(xt, tlosses, color='g', label='Training loss')\n","ax.plot(xv, vlosses, color='r', label='Validation loss')\n","\n","\n","# Best epoch\n","xb, yb = best_epoch, vlosses[int(best_epoch / epoch_freq)]\n","ax.plot(xb, yb,'bo')\n","ax.annotate('{}'.format(best_epoch),\n","            (xb, yb),\n","            textcoords=\"offset points\", # how to position the text\n","            xytext=(0,20), # distance from text to points (x,y)\n","            ha='center')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","ax.legend()\n","plt.show()\n","fig.savefig(os.path.join(fig_dir, 'loss-plot.png'))"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"train-autoencoder.ipynb","provenance":[{"file_id":"1WzsT1DOkzkQN5wroewQjngPuh8XsAtvE","timestamp":1632321587364}],"toc_visible":true,"authorship_tag":"ABX9TyOASRcCT49qYVek3SOznemY"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}