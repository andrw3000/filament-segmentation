{"cells":[{"cell_type":"markdown","metadata":{"id":"E6oq8m1yjrBr"},"source":["# Training an image segmentation model"]},{"cell_type":"markdown","metadata":{"id":"SVtdO3qtHKj8"},"source":["Organise data directories containing training data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0eE2aYn4HDwa"},"outputs":[],"source":["import os\n","import sys\n","import h5py  # !pip install pyyaml h5py\n","import time\n","import numpy as np\n","from tqdm.notebook import tqdm\n","import tensorflow as tf\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","\n","\n","# Mount Google Drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","\n","# Automatically reload imported programmes\n","%load_ext autoreload\n","%autoreload 2\n","\n","\n","# Model/database choice\n","dataset_name = 'all-2D'  # Refers to data_file = 'tomograms2D/all'\n","data_file = 'tomograms2D/all'  # No leading/trailing `/`\n","augmentation_choice = 'full'  # Choose from 'zoom'/'full'/'none'\n","model_name = 'unet'  # U-Net/Autoenncoder: 'unet'/'ae'\n","loss_name = 'scce'  # Mean Squared Error/Sparse Cat Cross Entropy: 'mse'/'scce'\n","use_existing_weights = True  # If available\n","\n","\n","# Locate data and experiment\n","database_name = dataset_name + '-' + augmentation_choice\n","exp_name = model_name + '-' + database_name + '-' + loss_name\n","\n","\n","# Directories (ammend as necessary)\n","root_dir = '/content/gdrive/MyDrive/IDSAI/PROOF/filament-segmentation'\n","os.chdir(root_dir)  # Move to root_dir\n","sys.path.insert(0, root_dir)\n","\n","\n","# Add data to root directory and locate JSON file\n","data_dir = os.path.join(root_dir, 'data/' + data_file)\n","image_path = os.path.join(data_dir, 'png-original')\n","masks_path = os.path.join(data_dir, 'png-masks/semantic/*.png')\n","\n","\n","# New training and validation files\n","train_dir = os.path.join(root_dir, 'data/databases/' + database_name + '/train')\n","valid_dir = os.path.join(root_dir, 'data/databases/' + database_name + '/valid')\n","\n","\n","# Checkpoints\n","checkpoint_dir = os.path.join(root_dir, 'checkpoints')\n","if not os.path.exists(checkpoint_dir):\n","    os.makedirs(checkpoint_dir)\n","best_weights_path = os.path.join(checkpoint_dir, exp_name + '.h5')\n","\n","\n","# Figure Outputs\n","fig_dir = os.path.join(root_dir, 'outputs/train-' + exp_name)\n","os.makedirs('outputs', exist_ok=True)\n","os.makedirs(fig_dir, exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"v8ErMPtHKAPh"},"source":["Choose network parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G6jeXNsuDUrq"},"outputs":[],"source":["lr = 0.0001\n","epoch_freq = 10\n","num_epochs = 300"]},{"cell_type":"markdown","metadata":{"id":"KfF-BJQpJ2XU"},"source":["Assert GPU/TPU and RAM capability."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kqp1c0wdt2rF"},"outputs":[],"source":["%%script false\n","# GPU info\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","   print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Btu9gegjuAPw"},"outputs":[],"source":["%%script false\n","# TPU initialisation for tensorflow 2.X\n","resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n","tf.config.experimental_connect_to_cluster(resolver)\n","tf.tpu.experimental.initialize_tpu_system(resolver)\n","print(\"All devices: \", tf.config.list_logical_devices('TPU'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RYbH70JJtJiE"},"outputs":[],"source":["%%script false\n","## RAM availability\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"markdown","metadata":{"id":"-EMTSZfGGvMk"},"source":["## Load or import data"]},{"cell_type":"markdown","metadata":{"id":"lrziffVM2Ze_"},"source":["Data choice."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7LoUP8eq2Y1k"},"outputs":[],"source":["batch_size = 10\n","shuffle_on = True\n","\n","if augmentation_choice == 'zoom':\n","    num_patches = 20  # Subsample taining data before augmenting\n","    num_duplicates = 1\n","    apply_augmentation = True\n","\n","elif augmentation_choice == 'full':\n","    num_patches = 1\n","    num_duplicates = 30  # Duplicate full image to augment\n","    apply_augmentation = True\n","    \n","elif augmentation_choice == 'none':\n","    num_patches = 1\n","    num_duplicates = 1\n","    apply_augmentation = False  # No augmentation (small dataset)\n","\n","else:\n","    raise ValueError('Please select a pre-defined `augmentation_choice`.')"]},{"cell_type":"markdown","metadata":{"id":"NvyKchS_4XGV"},"source":["Load from file or compute dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MV7YwZNnD4HR"},"outputs":[],"source":["print('\\nLoading data...')\n","if not os.path.exists(train_dir) and not os.path.exists(valid_dir):\n","\n","    from loader import augment_data, get_data\n","    train_imgs, train_msks, valid_imgs, valid_msks, _, _ = \\\n","        get_data(path_train_imgs=image_path,\n","                 path_train_msks=masks_path,\n","                 path_valid_imgs='',\n","                 path_valid_msks='',\n","                 train_frac=0.8,\n","                 valid_frac=0.1,\n","                 image_size=[256, 256],\n","                 num_patches_per_image=num_patches,\n","                 num_duplicates_per_image=num_duplicates,\n","                 )\n","        \n","    train_set, valid_set = augment_data(train_imgs,\n","                                        train_msks,\n","                                        valid_imgs,\n","                                        valid_msks,\n","                                        batch_size,\n","                                        one_hot=False,\n","                                        augment_on=apply_augmentation,\n","                                        shuffle_on=True,\n","                                        )\n","\n","    tf.data.experimental.save(train_set, train_dir)\n","    tf.data.experimental.save(valid_set, valid_dir)\n","    print('Data processed, loaded and saved.')\n","else:\n","    train_set = tf.data.experimental.load(train_dir)\n","    valid_set = tf.data.experimental.load(valid_dir)\n","    print('Data loaded from file.')\n","print('\\nTraining set length: ', len(train_set))\n","print('Validation set length: ', len(valid_set))\n","print('Batch size: ', batch_size)"]},{"cell_type":"markdown","metadata":{"id":"eTm0-PvNSXEX"},"source":["## Load model and optimisation strategy"]},{"cell_type":"markdown","metadata":{"id":"3VqzQw86SJy7"},"source":["Define optimisation strategy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k_IRKOQySIlD"},"outputs":[],"source":["# Learning rate\n","lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n","    lr, decay_steps=1000, decay_rate=0.75, staircase=True\n",")\n","\n","# Optimiser\n","optimiser = keras.optimizers.Adam(learning_rate=lr_schedule)"]},{"cell_type":"markdown","metadata":{"id":"QCK39Yy_4uX5"},"source":["Specify loss function and training metrics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-RWXutF4wBr"},"outputs":[],"source":["if loss_name == 'mse':\n","    loss_fn = keras.losses.MeanSquaredError()\n","    train_metric = keras.metrics.MeanSquaredError()\n","    val_metric = keras.metrics.MeanSquaredError()\n","    num_classes = 1  # Integer class encoding\n","\n","elif loss_name == 'scce':\n","    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    train_metric = keras.metrics.SparseCategoricalAccuracy()\n","    val_metric = keras.metrics.SparseCategoricalAccuracy()\n","    num_classes = 2  # One-hot encoded classes\n","\n","else:\n","    raise ValueError('Please select a pre-defined `loss_fn` choice.')"]},{"cell_type":"markdown","metadata":{"id":"tbrgiKwwIQKT"},"source":["Instantiate model and load the `best-weights.h5` from previous training sessions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mu5o9IU7IHCs"},"outputs":[],"source":["# Instantiate model\n","if model_name == 'unet':\n","    from models import get_unet_model\n","    model = get_unet_model(\n","        image_size=(256, 256), num_colour_channels=1, num_classes=num_classes,\n","    )\n","\n","elif model_name == 'ae':\n","    from models import get_autoencoder_model\n","    model = get_autoencoder_model(\n","        image_size=(256, 256), num_colour_channels=1, num_classes=num_classes,\n","    )\n","\n","else:\n","    raise ValueError('Please select a pre-defined `model_name` choice.')\n","\n","# Call model to build and initialise weights\n","example_image, _ = next(iter(train_set))\n","model(example_image)\n","\n","if os.path.exists(best_weights_path) and use_existing_weights:\n","    model.load_weights(best_weights_path)\n","    print('Model loaded with previous `best weights`.')\n","\n","else:\n","    print('Initialising model randomly.')\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"GGhe1-BUITMu"},"source":["## Training Routine"]},{"cell_type":"markdown","metadata":{"id":"7nlkrOqxD1JM"},"source":["Train/test steps."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdX_cq4p6n8h"},"outputs":[],"source":["@tf.function\n","def train_step(x, y, metric, loss_fn, opt):\n","    with tf.GradientTape() as tape:\n","        y_pred = model(x, training=True)\n","        loss = loss_fn(y, y_pred)\n","    grads = tape.gradient(loss, model.trainable_weights)\n","    opt.apply_gradients(zip(grads, model.trainable_weights))\n","    metric.update_state(y, y_pred)  # Update training metric\n","    return loss\n","\n","@tf.function\n","def test_step(x, y, loss_fn, metric=None):\n","    y_pred = model(x, training=False)  # Run prediction\n","    loss = loss_fn(y, y_pred)  # Compute loss\n","    if metric is not None:\n","        metric.update_state(y, y_pred)  # Update val metrics\n","    return y_pred, loss"]},{"cell_type":"markdown","metadata":{"id":"VWljORP_4C_s"},"source":["Iterate training session."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fs80WAo3SH2e"},"outputs":[],"source":["# Iterate training session\n","train_losses, train_acc, val_losses, val_acc = [], [], [], []\n","best_epoch = 0\n","\n","for epoch in range(num_epochs):\n","    start_time = time.time()\n","\n","    if epoch == 0:\n","        print('Saving model weights at epoch {:d}.'.format(epoch))\n","        model.save_weights(best_weights_path, save_format='h5')\n","\n","    # Iterate over the batches of the dataset\n","    for step, (x_train, y_train) in enumerate(train_set):\n","\n","        # Take training step, optimising model and computing loss\n","        loss = train_step(x_train, y_train, train_metric, loss_fn, optimiser)\n","        train_losses.extend(np.array(loss).flatten())  # Store accumulation\n","\n","        last_train_data = (x_train, y_train)  # Save last value for plotting\n","\n","\n","    # Display metrics at the end of each epoch\n","    train_acc.append(train_metric.result())\n","    train_metric.reset_states()\n","    print(\"Epoch {:d} training loss: {:.4f}\"\n","          .format(epoch, float(train_acc[-1]))\n","    )\n","\n","\n","    # Validate and checkpoint every `epoch_freq` epochs\n","    if epoch % epoch_freq == 0:\n","\n","        for x_val, y_val in valid_set:\n","            ypred_val, loss_val = test_step(x_val, y_val, loss_fn, val_metric)\n","\n","\n","\n","        if epoch > 0 and all(loss_val <= loss for loss in val_losses):\n","            best_epoch = epoch\n","            print('\\nValidation loss decrease from {:.3f} to {:.3f} '\n","                  'over epochs {:d} to {:d}.'.format(val_losses[-1],\n","                                                     loss_val,\n","                                                     epoch - epoch_freq,\n","                                                     epoch,\n","                                                     )\n","            )\n","\n","            print('Saving initial model weights at epoch {:d}.'.format(epoch))\n","            model.save_weights(best_weights_path, save_format='h5')\n","\n","        else:\n","            print(\n","                '\\nNo validation loss improvement at epoch {:d}.'.format(epoch)\n","            )\n","\n","        # Update validation metrics\n","        val_losses.append(loss_val)\n","        val_acc.append(val_metric.result())\n","        val_metric.reset_states()\n","        print(\"Epoch {:d} validation loss: {:.4f}\"\n","              .format(epoch, float(val_acc[-1]))\n","              )\n","        print(\"Epoch {:d} running time: {:.2f}s\"\n","              .format(epoch, time.time() - start_time)\n","        )\n","\n","        # Training predicition for printing\n","        x_train, y_train = last_train_data\n","        ypred_train, loss_train = test_step(x_train, y_train, loss_fn)\n","\n","        # Graphical Output\n","        print('\\nTraining Results.')\n","        print('Range of mask energies: {min:.3f} to {max:.3f}'\n","            .format(min=np.min(ypred_train[0, :, :, 0]),\n","                    max=np.max(ypred_train[0, :, :, 0]),\n","                    )\n","            )\n","        fig, ax = plt.subplots(1, 3, constrained_layout=True)\n","        ax[0].imshow(x_train[1, :, :, 0])\n","        ax[0].set_title('Input')\n","        ax[1].imshow(y_train[1, :, :, 0])\n","        ax[1].set_title('Mask')\n","        ax[2].imshow(ypred_train[1, :, :, 0])\n","        ax[2].set_title('Prediction')\n","        plt.show()\n","        fig.savefig(os.path.join(fig_dir, 'train_epoch{:04d}'.format(epoch)))\n","\n","        print('\\nValidation Results.')\n","        print('Range of mask energies: {min:.3f} to {max:.3f}'\n","            .format(min=np.min(ypred_val[0, :, :, 0]),\n","                    max=np.max(ypred_val[0, :, :, 0]),\n","                    )\n","            )\n","        fig, ax = plt.subplots(1, 3, constrained_layout=True)\n","        ax[0].imshow(x_val[3, :, :, 0])\n","        ax[0].set_title('Input')\n","        ax[1].imshow(y_val[3, :, :, 0])\n","        ax[1].set_title('Mask')\n","        ax[2].imshow(ypred_val[3, :, :, 0])\n","        ax[2].set_title('Prediction')\n","        plt.show()\n","        fig.savefig(os.path.join(fig_dir, 'valid_epoch{:04d}'.format(epoch)))\n","        print('\\n')"]},{"cell_type":"code","source":["# Generate loss plot\n","fig, ax = plt.subplots(1, 1)\n","\n","xt = np.linspace(\n","    0, num_epochs - (1/len(train_set)), num_epochs * len(train_set)\n",")\n","\n","xv = np.linspace(0, num_epochs - epoch_freq, int(num_epochs / epoch_freq))\n","\n","ax.plot(xt, train_losses, color='g', label='Training loss')\n","ax.plot(xv, val_losses, color='r', label='Validation loss')\n","\n","\n","# Best epoch\n","xb, yb = best_epoch, val_losses[int(best_epoch / epoch_freq)]\n","ax.plot(xb, yb,'bo')\n","ax.annotate('{}'.format(best_epoch),\n","            (xb, yb),\n","            textcoords=\"offset points\", # how to position the text\n","            xytext=(0,20), # distance from text to points (x,y)\n","            ha='center')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","ax.legend()\n","ax.set_title('Loss plot')\n","plt.show()\n","fig.savefig(os.path.join(fig_dir, 'loss-plot.png'))"],"metadata":{"id":"RmgjO6rj1uWP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQsGTswbDV5a"},"outputs":[],"source":["# Generate metric plot\n","fig, ax = plt.subplots(1, 1)\n","\n","xt = np.linspace(0, num_epochs-1, num_epochs)\n","xv = np.linspace(0, num_epochs - epoch_freq, int(num_epochs / epoch_freq))\n","\n","ax.plot(xt, train_acc, color='g', label='Training loss')\n","ax.plot(xv, val_acc, color='r', label='Validation loss')\n","\n","\n","# Best epoch\n","xb, yb = best_epoch, val_acc[int(best_epoch / epoch_freq)]\n","ax.plot(xb, yb,'bo')\n","ax.annotate('{}'.format(best_epoch),\n","            (xb, yb),\n","            textcoords=\"offset points\", # how to position the text\n","            xytext=(0,20), # distance from text to points (x,y)\n","            ha='center')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","ax.legend()\n","ax.set_title('Average loss plot')\n","plt.show()\n","fig.savefig(os.path.join(fig_dir, 'avg-loss-plot.png'))"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"train-model.ipynb","provenance":[{"file_id":"1WzsT1DOkzkQN5wroewQjngPuh8XsAtvE","timestamp":1632321587364}],"toc_visible":true,"authorship_tag":"ABX9TyMLqd959Iu9qqfTaS03Dzyq"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}