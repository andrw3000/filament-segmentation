{"cells":[{"cell_type":"markdown","metadata":{"id":"G5rFvdxUwxxC"},"source":["# Demo: database\n","Create and seperate a TF database with augmented images and files for testing, training and validation."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2908,"status":"ok","timestamp":1642680675436,"user":{"displayName":"Andrew Corbett","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfWvSpJcF8ufZCpMOCz9qkF5qYKUu3WQYoyFxl=s64","userId":"01044198545578387343"},"user_tz":0},"id":"mcEa4RYsTBT4","outputId":"569842e2-87b1-4ec5-9744-98acc3b4252d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["import os\n","import sys\n","from google.colab import drive\n","\n","\n","# Mount Google Drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","# Automatically reload imported programmes\n","%load_ext autoreload\n","%autoreload 2\n","\n","\n","# Database name\n","dataset_name = 'all2D'  # Refers to data_file = 'tomograms2D/all'\n","augmentation_choice = 'none'  # Choose from 'zoom'/'full'/'none'\n","database_name = dataset_name + '-' + augmentation_choice\n","\n","\n","# Directories (ammend as necessary)\n","root_dir = '/content/gdrive/MyDrive/IDSAI/PROOF/filament-segmentation'\n","os.chdir(root_dir)  # Move to root_dir\n","sys.path.insert(0, root_dir)\n","\n","\n","# RAW data location\n","data_file = 'tomograms2D/all'  # No leading/trailing `/`\n","data_dir = os.path.join(root_dir, 'data/' + data_file)\n","image_path = os.path.join(data_dir, 'png-original')\n","masks_path = os.path.join(data_dir, 'png-masks/semantic/*.png')\n","\n","\n","# New training and validation files\n","train_dir = os.path.join(root_dir, 'data/databases/' + database_name + '/train')\n","valid_dir = os.path.join(root_dir, 'data/databases/' + database_name + '/valid')"]},{"cell_type":"markdown","metadata":{"id":"uByiLbl6x8qg"},"source":["## Create TF database for training"]},{"cell_type":"markdown","source":["Set parameters for augmentation."],"metadata":{"id":"iMH8ps8-DtDd"}},{"cell_type":"code","source":["batch_size = 10\n","shuffle_on = True\n","\n","if augmentation_choice == 'zoom':\n","    num_patches = 20  # Subsample taining data before augmenting\n","    num_duplicates = 1\n","    apply_augmentation = True\n","\n","elif augmentation_choice == 'full':\n","    num_patches = 1\n","    num_duplicates = 30  # Duplicate full image to augment\n","    apply_augmentation = True\n","    \n","elif augmentation_choice == 'none':\n","    num_patches = 1\n","    num_duplicates = 1\n","    apply_augmentation = False  # No augmentation (small dataset)\n","\n","else:\n","    raise ValueError('Please select a pre-defined `augmentation_choice`.')"],"metadata":{"id":"34DycAuBDsHl","executionInfo":{"status":"ok","timestamp":1642680675438,"user_tz":0,"elapsed":8,"user":{"displayName":"Andrew Corbett","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfWvSpJcF8ufZCpMOCz9qkF5qYKUu3WQYoyFxl=s64","userId":"01044198545578387343"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Load and save dataset."],"metadata":{"id":"D7KotvhZDwV2"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"93oYAireWI-K","executionInfo":{"status":"ok","timestamp":1642681434218,"user_tz":0,"elapsed":758787,"user":{"displayName":"Andrew Corbett","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfWvSpJcF8ufZCpMOCz9qkF5qYKUu3WQYoyFxl=s64","userId":"01044198545578387343"}},"outputId":"69d2ef1c-0ab2-40a8-ae5d-f9c18bb25697"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Processing data...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 186/186 [12:14<00:00,  3.95s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Data processed and saved.\n","\n","Training set length:  15\n","Validation set length:  2\n"]}],"source":["import tensorflow as tf\n","from loader import augment_data, get_data\n","\n","print('\\nProcessing data...')\n","train_imgs, train_msks, valid_imgs, valid_msks, _, _ = \\\n","    get_data(path_train_imgs=image_path,\n","                path_train_msks=masks_path,\n","                path_valid_imgs='',\n","                path_valid_msks='',\n","                train_frac=0.8,\n","                valid_frac=0.1,\n","                image_size=[256, 256],\n","                num_patches_per_image=num_patches,\n","                num_duplicates_per_image=num_duplicates,\n","                )\n","    \n","train_set, valid_set = augment_data(train_imgs,\n","                                    train_msks,\n","                                    valid_imgs,\n","                                    valid_msks,\n","                                    batch_size,\n","                                    one_hot=False,\n","                                    augment_on=apply_augmentation,\n","                                    shuffle_on=True,\n","                                    )\n","\n","tf.data.experimental.save(train_set, train_dir)\n","tf.data.experimental.save(valid_set, valid_dir)\n","\n","print('Data processed and saved.\\n')\n","print('Training set length: ', len(train_set))\n","print('Validation set length: ', len(valid_set))"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"load-database.ipynb","toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyMEM5DnaCjHY+zzBNDN4kwq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}